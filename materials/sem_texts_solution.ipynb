{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар по обработке текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Методы обработки текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примеры задач автоматической обработки текстов:\n",
    "\n",
    "- классификация текстов\n",
    "\n",
    "    - анализ тональности\n",
    "    - фильтрация спама\n",
    "    - по теме или жанру\n",
    "\n",
    "- машинный перевод\n",
    "\n",
    "- распознавание речи\n",
    "\n",
    "- извлечение информации\n",
    "\n",
    "    - именованные сущности\n",
    "    - факты и события\n",
    "\n",
    "- кластеризация текстов\n",
    "\n",
    "- оптическое распознавание символов\n",
    "\n",
    "- проверка правописания\n",
    "\n",
    "- вопросно-ответные системы\n",
    "\n",
    "- суммаризация текстов\n",
    "\n",
    "- генерация текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одни из классических методов для работы с текстами:\n",
    "\n",
    "- токенизация\n",
    "\n",
    "- лемматизация / стемминг\n",
    "\n",
    "- удаление стоп-слов\n",
    "\n",
    "- векторное представление текстов (bag of words и TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Что почитать:_\n",
    "\n",
    "- Jurafsky, Martin: Speech and Language Processing (2nd or 3rd Edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация\n",
    "\n",
    "Токенизировать -- значит, поделить текст на слова, или *токены*.\n",
    "\n",
    "Самый наивный способ токенизировать текст -- разделить с помощью `split`. Но `split` упускает очень много всего, например, банально не отделяет пунктуацию от слов. Кроме этого, есть ещё много менее тривиальных проблем. Поэтому лучше использовать готовые токенизаторы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/user/anaconda3/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: six in /Users/user/anaconda3/anaconda3/lib/python3.6/site-packages (from nltk)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example = 'Но не каждый хочет что-то исправлять:('"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Но', 'не', 'каждый', 'хочет', 'что-то', 'исправлять:(']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "# без этой ячейки следующая ячейка выдаст ошибку о том, что \n",
    "# ресурс english.pickle не найден"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Но', 'не', 'каждый', 'хочет', 'что-то', 'исправлять', ':', '(']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В nltk вообще есть довольно много токенизаторов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BlanklineTokenizer',\n",
       " 'LineTokenizer',\n",
       " 'MWETokenizer',\n",
       " 'PunktSentenceTokenizer',\n",
       " 'RegexpTokenizer',\n",
       " 'ReppTokenizer',\n",
       " 'SExprTokenizer',\n",
       " 'SpaceTokenizer',\n",
       " 'StanfordSegmenter',\n",
       " 'StanfordTokenizer',\n",
       " 'TabTokenizer',\n",
       " 'TextTilingTokenizer',\n",
       " 'ToktokTokenizer',\n",
       " 'TreebankWordTokenizer',\n",
       " 'TweetTokenizer',\n",
       " 'WhitespaceTokenizer']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "dir(tokenize)[:16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Но', 'не', 'каждый', 'хочет', 'что-то', 'исправлять', ':(']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tw = TweetTokenizer()\n",
    "tw.tokenize(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['London', 'is', 'a', 'capital', 'of', 'Great', 'Britain']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_example = \"London is a capital of Great Britain\" \n",
    "tw.tokenize(en_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Что почитать:_\n",
    "\n",
    "- http://mlexplained.com/2019/11/06/a-deep-dive-into-the-wonderful-world-of-preprocessing-in-nlp/\n",
    "- https://blog.floydhub.com/tokenization-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стоп-слова и пунктуация\n",
    "\n",
    "*Стоп-слова* -- это слова, которые часто встречаются практически в любом тексте и ничего интересного не говорят о конретном документе, то есть играют роль шума. Поэтому их принято убирать. По той же причине убирают и пунктуацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Больше русских стоп-слов здесь: https://github.com/stopwords-iso/stopwords-ru/blob/master/stopwords-ru.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noise = stopwords.words('russian') + list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['и',\n",
       " 'в',\n",
       " 'во',\n",
       " 'не',\n",
       " 'что',\n",
       " 'он',\n",
       " 'на',\n",
       " 'я',\n",
       " 'с',\n",
       " 'со',\n",
       " 'как',\n",
       " 'а',\n",
       " 'то',\n",
       " 'все',\n",
       " 'она',\n",
       " 'так',\n",
       " 'его',\n",
       " 'но',\n",
       " 'да',\n",
       " 'ты',\n",
       " 'к',\n",
       " 'у',\n",
       " 'же',\n",
       " 'вы',\n",
       " 'за',\n",
       " 'бы',\n",
       " 'по',\n",
       " 'только',\n",
       " 'ее',\n",
       " 'мне',\n",
       " 'было',\n",
       " 'вот',\n",
       " 'от',\n",
       " 'меня',\n",
       " 'еще',\n",
       " 'нет',\n",
       " 'о',\n",
       " 'из',\n",
       " 'ему',\n",
       " 'теперь',\n",
       " 'когда',\n",
       " 'даже',\n",
       " 'ну',\n",
       " 'вдруг',\n",
       " 'ли',\n",
       " 'если',\n",
       " 'уже',\n",
       " 'или',\n",
       " 'ни',\n",
       " 'быть',\n",
       " 'был',\n",
       " 'него',\n",
       " 'до',\n",
       " 'вас',\n",
       " 'нибудь',\n",
       " 'опять',\n",
       " 'уж',\n",
       " 'вам',\n",
       " 'ведь',\n",
       " 'там',\n",
       " 'потом',\n",
       " 'себя',\n",
       " 'ничего',\n",
       " 'ей',\n",
       " 'может',\n",
       " 'они',\n",
       " 'тут',\n",
       " 'где',\n",
       " 'есть',\n",
       " 'надо',\n",
       " 'ней',\n",
       " 'для',\n",
       " 'мы',\n",
       " 'тебя',\n",
       " 'их',\n",
       " 'чем',\n",
       " 'была',\n",
       " 'сам',\n",
       " 'чтоб',\n",
       " 'без',\n",
       " 'будто',\n",
       " 'чего',\n",
       " 'раз',\n",
       " 'тоже',\n",
       " 'себе',\n",
       " 'под',\n",
       " 'будет',\n",
       " 'ж',\n",
       " 'тогда',\n",
       " 'кто',\n",
       " 'этот',\n",
       " 'того',\n",
       " 'потому',\n",
       " 'этого',\n",
       " 'какой',\n",
       " 'совсем',\n",
       " 'ним',\n",
       " 'здесь',\n",
       " 'этом',\n",
       " 'один',\n",
       " 'почти',\n",
       " 'мой',\n",
       " 'тем',\n",
       " 'чтобы',\n",
       " 'нее',\n",
       " 'сейчас',\n",
       " 'были',\n",
       " 'куда',\n",
       " 'зачем',\n",
       " 'всех',\n",
       " 'никогда',\n",
       " 'можно',\n",
       " 'при',\n",
       " 'наконец',\n",
       " 'два',\n",
       " 'об',\n",
       " 'другой',\n",
       " 'хоть',\n",
       " 'после',\n",
       " 'над',\n",
       " 'больше',\n",
       " 'тот',\n",
       " 'через',\n",
       " 'эти',\n",
       " 'нас',\n",
       " 'про',\n",
       " 'всего',\n",
       " 'них',\n",
       " 'какая',\n",
       " 'много',\n",
       " 'разве',\n",
       " 'три',\n",
       " 'эту',\n",
       " 'моя',\n",
       " 'впрочем',\n",
       " 'хорошо',\n",
       " 'свою',\n",
       " 'этой',\n",
       " 'перед',\n",
       " 'иногда',\n",
       " 'лучше',\n",
       " 'чуть',\n",
       " 'том',\n",
       " 'нельзя',\n",
       " 'такой',\n",
       " 'им',\n",
       " 'более',\n",
       " 'всегда',\n",
       " 'конечно',\n",
       " 'всю',\n",
       " 'между',\n",
       " '!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простой пример удаления стоп-слов из строки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Но', 'не', 'каждый', 'хочет', 'что-то', 'исправлять', ':', '(']\n",
      "['Но', 'каждый', 'хочет', 'что-то', 'исправлять']\n"
     ]
    }
   ],
   "source": [
    "tokenized_example = word_tokenize(example)\n",
    "filtered_example = [word for word in tokenized_example if not word in noise]\n",
    "print(tokenized_example)\n",
    "print(filtered_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лемматизация\n",
    "\n",
    "Лемматизация – это сведение разных форм одного слова к начальной форме – *лемме*. Например, токены «пью», «пил», «пьет» перейдут в «пить». Почему это хорошо?\n",
    "* Во-первых, мы хотим рассматривать как отдельный признак каждое *слово*, а не каждую его отдельную форму.\n",
    "* Во-вторых, некоторые стоп-слова стоят только в начальной форме, и без лемматизации выкидываем мы только её.\n",
    "\n",
    "Для английского языка можно пользоваться nltk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat running ran cactus cactus cactus community community\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "en_example = \"cats running ran cactus cactuses cacti community communities\"\n",
    "en_tokenized = tw.tokenize(en_example)\n",
    "text_lemmatized = [lemmatizer.lemmatize(w) for w in en_tokenized]\n",
    "print(' '.join(text_lemmatized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для русского есть два хороших лемматизатора: mystem и pymorphy.\n",
    "\n",
    "### [Mystem](https://tech.yandex.ru/mystem/)\n",
    "Как с ним работать:\n",
    "* можно скачать mystem и запускать [из терминала с разными параметрами](https://tech.yandex.ru/mystem/doc/)\n",
    "* [pymystem3](https://pythonhosted.org/pymystem3/pymystem3.html) - обертка для питона, работает медленнее, но это удобно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymystem3\n",
      "  Downloading https://files.pythonhosted.org/packages/00/8c/98b43c5822620458704e187a1666616c1e21a846ede8ffda493aabe11207/pymystem3-0.2.0-py3-none-any.whl\n",
      "Requirement already satisfied: requests in /Users/user/anaconda3/anaconda3/lib/python3.6/site-packages (from pymystem3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/user/anaconda3/anaconda3/lib/python3.6/site-packages (from requests->pymystem3)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /Users/user/anaconda3/anaconda3/lib/python3.6/site-packages (from requests->pymystem3)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /Users/user/anaconda3/anaconda3/lib/python3.6/site-packages (from requests->pymystem3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/user/anaconda3/anaconda3/lib/python3.6/site-packages (from requests->pymystem3)\n",
      "Installing collected packages: pymystem3\n",
      "Successfully installed pymystem3-0.2.0\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing mystem to /Users/user/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.1-macosx.tar.gz\n"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem\n",
    "mystem_analyzer = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы инициализировали Mystem c дефолтными параметрами. А вообще параметры есть такие:\n",
    "* mystem_bin - путь к `mystem`, если их несколько\n",
    "* grammar_info - нужна ли грамматическая информация или только леммы (по умолчанию нужна)\n",
    "* disambiguation - нужно ли снятие омонимии - дизамбигуация (по умолчанию нужна)\n",
    "* entire_input - нужно ли сохранять в выводе все (пробелы всякие, например), или можно выкинуть (по умолчанию оставляется все)\n",
    "\n",
    "Методы Mystem принимают строку, токенизатор вшит внутри. Можно, конечно, и пословно анализировать, но тогда он не сможет учитывать контекст.\n",
    "\n",
    "Можно просто лемматизировать текст:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['но', ' ', 'не', ' ', 'каждый', ' ', 'хотеть', ' ', 'что-то', ' ', 'исправлять', ':(\\n']\n"
     ]
    }
   ],
   "source": [
    "print(mystem_analyzer.lemmatize(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно запросить больше информации о каждом слове:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'analysis': [{'lex': 'но', 'wt': 0.9998906299, 'gr': 'CONJ='}], 'text': 'Но'}, {'text': ' '}, {'analysis': [{'lex': 'не', 'wt': 1, 'gr': 'PART='}], 'text': 'не'}, {'text': ' '}, {'analysis': [{'lex': 'каждый', 'wt': 0.9985975799, 'gr': 'APRO=(вин,ед,муж,неод|им,ед,муж)'}], 'text': 'каждый'}, {'text': ' '}, {'analysis': [{'lex': 'хотеть', 'wt': 1, 'gr': 'V,несов,пе=непрош,ед,изъяв,3-л'}], 'text': 'хочет'}, {'text': ' '}, {'analysis': [{'lex': 'что-то', 'wt': 1, 'gr': 'SPRO,ед,сред,неод=(вин|им)'}], 'text': 'что-то'}, {'text': ' '}, {'analysis': [{'lex': 'исправлять', 'wt': 1, 'gr': 'V,пе=инф,несов'}], 'text': 'исправлять'}, {'text': ':(\\n'}]\n"
     ]
    }
   ],
   "source": [
    "print(mystem_analyzer.analyze(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Pymorphy](http://pymorphy2.readthedocs.io/en/latest/)\n",
    "Это модуль на питоне, довольно быстрый и с множеством функций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 864kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting pymorphy2-dicts<3.0,>=2.4 (from pymorphy2)\n",
      "  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.1MB 152kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting dawg-python>=0.7 (from pymorphy2)\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
      "Collecting docopt>=0.6 (from pymorphy2)\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/55/8f8cab2afd404cf578136ef2cc5dfb50baa1761b68c9da1fb1e4eed343c9/docopt-0.6.2.tar.gz\n",
      "Building wheels for collected packages: docopt\n",
      "  Running setup.py bdist_wheel for docopt ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/user/Library/Caches/pip/wheels/9b/04/dd/7daf4150b6d9b12949298737de9431a324d4b797ffd63f526e\n",
      "Successfully built docopt\n",
      "Installing collected packages: pymorphy2-dicts, dawg-python, docopt, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pymorphy2-dicts in /Users/user/anaconda3/anaconda3/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: DAWG-Python in /Users/user/anaconda3/anaconda3/lib/python3.6/site-packages\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2\n",
    "!pip install pymorphy2-dicts\n",
    "!pip install DAWG-Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "pymorphy2_analyzer = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pymorphy2 работает с отдельными словами. Если дать ему на вход предложение - он его просто не лемматизирует, т.к. не понимает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_example = tw.tokenize(example) \n",
    "# токенизатор Твиттера, который мы пробовали выше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Но', 'не', 'каждый', 'хочет', 'что-то', 'исправлять', ':(']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='хочет', tag=OpencorporaTag('VERB,impf,tran sing,3per,pres,indc'), normal_form='хотеть', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'хочет', 2999, 5),))]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ana = pymorphy2_analyzer.parse(tokenized_example[3])\n",
    "ana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'хотеть'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ana[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['но', 'не', 'каждый', 'хотеть', 'что-то', 'исправлять', ':(']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[pymorphy2_analyzer.parse(word)[0].normal_form \\\n",
    " for word in tokenized_example]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если у слова несколько вариантов лемматизации, pymorphy выдаст список всех вариантов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='мыла', tag=OpencorporaTag('NOUN,inan,neut sing,gent'), normal_form='мыло', score=0.333333, methods_stack=((<DictionaryAnalyzer>, 'мыла', 54, 1),)),\n",
       " Parse(word='мыла', tag=OpencorporaTag('VERB,impf,tran femn,sing,past,indc'), normal_form='мыть', score=0.333333, methods_stack=((<DictionaryAnalyzer>, 'мыла', 1813, 8),)),\n",
       " Parse(word='мыла', tag=OpencorporaTag('NOUN,inan,neut plur,nomn'), normal_form='мыло', score=0.166666, methods_stack=((<DictionaryAnalyzer>, 'мыла', 54, 6),)),\n",
       " Parse(word='мыла', tag=OpencorporaTag('NOUN,inan,neut plur,accs'), normal_form='мыло', score=0.166666, methods_stack=((<DictionaryAnalyzer>, 'мыла', 54, 9),))]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pymorphy2_analyzer.parse(\"мыла\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mystem vs. pymorphy\n",
    "\n",
    "1) Mystem работает медленно под windows на больших текстах\n",
    "\n",
    "2) *Снятие омонимии*. Mystem умеет снимать омонимию по контексту (хотя не всегда преуспевает), pymorphy2 берет на вход одно слово и соответственно вообще не умеет дизамбигуировать по контексту."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analysis': [{'lex': 'сорок', 'wt': 0.8710292664, 'gr': 'NUM=(пр|дат|род|твор)'}], 'text': 'сорока'}\n",
      "{'analysis': [{'lex': 'сорока', 'wt': 0.1210970041, 'gr': 'S,жен,од=им,ед'}], 'text': 'Сорока'}\n"
     ]
    }
   ],
   "source": [
    "homonym1 = 'За время обучения я прослушал больше сорока курсов.'\n",
    "homonym2 = 'Сорока своровала блестящее украшение со стола.'\n",
    "mystem_analyzer = Mystem() # инициализируем объект с параметрами по умолчанию\n",
    "\n",
    "print(mystem_analyzer.analyze(homonym1)[-5])\n",
    "print(mystem_analyzer.analyze(homonym2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['сорока',\n",
       " ' ',\n",
       " 'своровать',\n",
       " ' ',\n",
       " 'блестящий',\n",
       " ' ',\n",
       " 'украшение',\n",
       " ' ',\n",
       " 'со',\n",
       " ' ',\n",
       " 'стол',\n",
       " '.',\n",
       " '\\n']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystem_analyzer.lemmatize(homonym2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стемминг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличие от лемматизации, при применении стемминга у всех слов отбрасываются аффиксы (окончания и суффиксы), что необязательно приводит слова к формам, существующим в рассматриваемом языке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "но не кажд хочет что-т исправля :(\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('russian')\n",
    "stemmed_example = [stemmer.stem(w) for w in tokenized_example]\n",
    "print(' '.join(stemmed_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In my younger and more vulnerable years my father gave me some advice that I've been turning over in my mind ever since.\n",
      "\"Whenever you feel like criticizing any one,\" he told me, \"just remember that all the people in this world haven't had the advantages that you've had.\"\n"
     ]
    }
   ],
   "source": [
    "text = \"In my younger and more vulnerable years my father gave me some advice that I've been turning over in my mind ever since.\\n\\\"Whenever you feel like criticizing any one,\\\" he told me, \\\"just remember that all the people in this world haven't had the advantages that you've had.\\\"\"\n",
    "print(text)\n",
    "text_tokenized = [w for w in word_tokenize(text) if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in my younger and more vulner year my father gave me some advic that i been turn over in my mind ever sinc whenev you feel like critic ani one he told me just rememb that all the peopl in this world have had the advantag that you had\n"
     ]
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "text_stemmed = [stemmer.stem(w) for w in text_tokenized]\n",
    "print(' '.join(text_stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Что почитать:_\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Stemming\n",
    "- https://en.wikipedia.org/wiki/Lemmatisation\n",
    "- https://www.datacamp.com/community/tutorials/stemming-lemmatization-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-words и TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но как же все-таки работать с текстами, используя стандартные методы машинного обучения? Нужна выборка!\n",
    "\n",
    "Модель bag-of-words: считаем, сколько раз каждое слово встретилось в тексте. Число слов = число признаков (число признаков одинаковое для всех текстов, много нулевых признаков). Вместо частоты встречаемости можно использовать другие характеристики, например, бинарная (встретилось слово / не встретилось) и другие."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = ['I like my cat.', 'My cat is the most perfect cat.', 'is this cat or is this bread?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I like my cat',\n",
       " 'My cat is the most perfect cat',\n",
       " 'is this cat or is this bread']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_tokenized = [' '.join([w for w in word_tokenize(t) if w.isalpha()]) for t in texts]\n",
    "texts_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cnt_vec = CountVectorizer()\n",
    "X = cnt_vec.fit_transform(texts_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bread', 'cat', 'is', 'like', 'most', 'my', 'or', 'perfect', 'the', 'this']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 14 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 2, 1, 0, 1, 1, 0, 1, 1, 0],\n",
       "       [1, 1, 2, 0, 0, 0, 1, 0, 0, 2]], dtype=int64)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bread</th>\n",
       "      <th>cat</th>\n",
       "      <th>is</th>\n",
       "      <th>like</th>\n",
       "      <th>most</th>\n",
       "      <th>my</th>\n",
       "      <th>or</th>\n",
       "      <th>perfect</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bread  cat  is  like  most  my  or  perfect  the  this\n",
       "0      0    1   0     1     0   1   0        0    0     0\n",
       "1      0    2   1     0     1   1   0        1    1     0\n",
       "2      1    1   2     0     0   0   1        0    0     2"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X.toarray(), columns=cnt_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что если слово часто встречается в одном тексте, но почти не встречается в других, то оно получает для данного текста большой вес, ровно так же, как и слова, которые часто встречаются в каждом тексте. Для того, чтобы разделять такие слова, можно использовать статистическую меру TF-IDF, характеризующую важность слова для конкретного текста. Для каждого слова из текста $d$ рассчитаем относительную частоту встречаемости в нем (Term Frequency):\n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{n_{td}}{\\sum\\limits_{k \\in d} n_{kd}},\n",
    "$$\n",
    "где $n_{td}$ - число вхождений слова $t$ в текст $d$.\n",
    "\n",
    "Также для каждого слова из текста $d$ рассчитаем обратную частоту встречаемости в корпусе текстов $D$ (Inverse Document Frequency):\n",
    "$$\n",
    "\\text{IDF}(t, D) = \\log\\left(\\frac{|D|}{|\\{d \\in D \\mid t \\in d\\}|}\\right).\n",
    "$$\n",
    "Здесь $\\{d \\in D \\mid t \\in d\\}$ - множество документов, в которых встречается слово $t$.\n",
    "Логарифмирование здесь проводится с целью уменьшить масштаб весов, ибо зачастую в корпусах присутствует очень много текстов.\n",
    "\n",
    "В итоге каждому слову $t$ из текста $d$ теперь можно присвоить вес\n",
    "$$\n",
    "\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\n",
    "$$\n",
    "Интерпретировать формулу выше несложно: действительно, __чем чаще__ данное слово встречается __в данном тексте__ и __чем реже в остальных__, тем __важнее__ оно для __этого__ текста.\n",
    "\n",
    "Отметим, что в качестве TF и IDF можно использовать другие определения (https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Definition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "X = tfidf_vec.fit_transform(texts_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bread', 'cat', 'is', 'like', 'most', 'my', 'or', 'perfect', 'the', 'this']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x10 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 14 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.42544054, 0.        , 0.72033345, 0.        ,\n",
       "        0.54783215, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.50130994, 0.32276391, 0.        , 0.42439575,\n",
       "        0.32276391, 0.        , 0.42439575, 0.42439575, 0.        ],\n",
       "       [0.33976626, 0.20067143, 0.516802  , 0.        , 0.        ,\n",
       "        0.        , 0.33976626, 0.        , 0.        , 0.67953252]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bread</th>\n",
       "      <th>cat</th>\n",
       "      <th>is</th>\n",
       "      <th>like</th>\n",
       "      <th>most</th>\n",
       "      <th>my</th>\n",
       "      <th>or</th>\n",
       "      <th>perfect</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.425441</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.720333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.547832</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.501310</td>\n",
       "      <td>0.322764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.424396</td>\n",
       "      <td>0.322764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.424396</td>\n",
       "      <td>0.424396</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.339766</td>\n",
       "      <td>0.200671</td>\n",
       "      <td>0.516802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.679533</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bread       cat        is      like      most        my        or  \\\n",
       "0  0.000000  0.425441  0.000000  0.720333  0.000000  0.547832  0.000000   \n",
       "1  0.000000  0.501310  0.322764  0.000000  0.424396  0.322764  0.000000   \n",
       "2  0.339766  0.200671  0.516802  0.000000  0.000000  0.000000  0.339766   \n",
       "\n",
       "    perfect       the      this  \n",
       "0  0.000000  0.000000  0.000000  \n",
       "1  0.424396  0.424396  0.000000  \n",
       "2  0.000000  0.000000  0.679533  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X.toarray(), columns=tfidf_vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что изменилось по сравнению с методом `CountVectorizer`? Интерпретируйте результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Что почитать:_\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "- https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-граммы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что такое n-граммы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если',), ('б',), ('мне',), ('платили',), ('каждый',), ('раз',)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'Если б мне платили каждый раз'.split()\n",
    "list(ngrams(sent, 1)) # униграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если', 'б'),\n",
       " ('б', 'мне'),\n",
       " ('мне', 'платили'),\n",
       " ('платили', 'каждый'),\n",
       " ('каждый', 'раз')]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sent, 2)) # биграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если', 'б', 'мне'),\n",
       " ('б', 'мне', 'платили'),\n",
       " ('мне', 'платили', 'каждый'),\n",
       " ('платили', 'каждый', 'раз')]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sent, 3)) # триграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если', 'б', 'мне', 'платили', 'каждый'),\n",
       " ('б', 'мне', 'платили', 'каждый', 'раз')]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sent, 5)) # ... пентаграммы?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В CountVectorizer можно задать ngram_range, чтобы получился мешок n-грамм:<br/>\n",
    "ngram_range=(1, 1) -- униграммы<br/>\n",
    "ngram_range=(3, 3) -- триграммы<br/>\n",
    "ngram_range=(1, 3) -- униграммы, биграммы и триграммы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача: классификация твитов по тональности\n",
    "\n",
    "У нас есть датасет из твитов, про каждый указано, как он эмоционально окрашен: положительно или отрицательно. Задача: предсказывать эмоциональную окраску.\n",
    "\n",
    "Классификацию по тональности используют, например, в рекомендательных системах, чтобы понять, понравилось ли людям кафе, кино, etc.\n",
    "\n",
    "[Источник данных](http://study.mokoron.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# считываем данные и заполняем общий датасет\n",
    "positive = pd.read_csv('https://github.com/blacKitten13/minor2020-iad4/raw/master/sem10_texts/positive.csv', \n",
    "                       sep=';', usecols=[3], names=['text'])\n",
    "positive['label'] = [1] * len(positive)\n",
    "negative = pd.read_csv('https://github.com/blacKitten13/minor2020-iad4/raw/master/sem10_texts/negative.csv', \\\n",
    "                       sep=';', usecols=[3], names=['text'])\n",
    "negative['label'] = [0] * len(negative)\n",
    "df = positive.append(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111918</th>\n",
       "      <td>Но не каждый хочет что то исправлять:( http://...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111919</th>\n",
       "      <td>скучаю так :-( только @taaannyaaa вправляет мо...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111920</th>\n",
       "      <td>Вот и в школу, в говно это идти уже надо(</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111921</th>\n",
       "      <td>RT @_Them__: @LisaBeroud Тауриэль, не грусти :...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111922</th>\n",
       "      <td>Такси везет меня на работу. Раздумываю приплат...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "111918  Но не каждый хочет что то исправлять:( http://...      0\n",
       "111919  скучаю так :-( только @taaannyaaa вправляет мо...      0\n",
       "111920          Вот и в школу, в говно это идти уже надо(      0\n",
       "111921  RT @_Them__: @LisaBeroud Тауриэль, не грусти :...      0\n",
       "111922  Такси везет меня на работу. Раздумываю приплат...      0"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    114911\n",
       "0    111923\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226834, 2)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.text, df.label, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание:\n",
    "1. Преобразуйте данные с помощью CountVectorizer (fit на train, transform на обеих выборках)\n",
    "1. Сколько признаков получилось?\n",
    "1. Напечатайте 10 любых слов (хранятся в векторайзере, см. соответствующую секцию)\n",
    "1. Обучите логистическую регрессию.\n",
    "1. Выведите качество на обучении и контроле.\n",
    "1. (опционально) выведите самые влиятельные признаки (согласно весам модели)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число признаков: 243421\n",
      "Пример слов: ['100р', '100респектов', '100тыс', '100хан', '101', '10110000', '102', '1020', '1023зс', '103']\n",
      "ACC: 0.765\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1))\n",
    "x_train_bow = vec.fit_transform(x_train) # bow -- bag of words (мешок слов)\n",
    "x_test_bow = vec.transform(x_test)\n",
    "print(\"Число признаков:\", x_train_bow.shape[1])\n",
    "print(\"Пример слов:\", vec.get_feature_names()[579:589])\n",
    "clf = LogisticRegression(max_iter=300, random_state=42)\n",
    "clf.fit(x_train_bow, y_train)\n",
    "pred = clf.predict(vec.transform(x_test))\n",
    "print('ACC: %.3f' % \\\n",
    "      accuracy_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fnames</th>\n",
       "      <th>w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64845</th>\n",
       "      <td>o_o</td>\n",
       "      <td>-5.485647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21179</th>\n",
       "      <td>cio_optimal</td>\n",
       "      <td>-4.407953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84049</th>\n",
       "      <td>to_over_kill</td>\n",
       "      <td>-4.373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26275</th>\n",
       "      <td>do_or_die_xxx</td>\n",
       "      <td>-4.166842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70243</th>\n",
       "      <td>prisonero_o</td>\n",
       "      <td>-4.156347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37639</th>\n",
       "      <td>horanso_on</td>\n",
       "      <td>-3.684225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72988</th>\n",
       "      <td>reno_oppa</td>\n",
       "      <td>-3.670594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7130</th>\n",
       "      <td>_do_or_die__</td>\n",
       "      <td>-3.553882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7833</th>\n",
       "      <td>_ooo_ooo__</td>\n",
       "      <td>-3.451456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64850</th>\n",
       "      <td>o_obnulyay</td>\n",
       "      <td>-3.403966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18514</th>\n",
       "      <td>boo_ohoo</td>\n",
       "      <td>-3.403440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169691</th>\n",
       "      <td>о_о</td>\n",
       "      <td>-3.372854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47678</th>\n",
       "      <td>kota_oo_oo</td>\n",
       "      <td>-3.285324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72178</th>\n",
       "      <td>radio_of_moon</td>\n",
       "      <td>-3.257457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53242</th>\n",
       "      <td>lponomarenko_o</td>\n",
       "      <td>-2.982029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170084</th>\n",
       "      <td>обидно</td>\n",
       "      <td>-2.814166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181498</th>\n",
       "      <td>печально</td>\n",
       "      <td>-2.734797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43355</th>\n",
       "      <td>july_to_october</td>\n",
       "      <td>-2.709523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6125</th>\n",
       "      <td>99</td>\n",
       "      <td>-2.640723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131530</th>\n",
       "      <td>жизньболь</td>\n",
       "      <td>-2.598712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64860</th>\n",
       "      <td>o_olshik</td>\n",
       "      <td>-2.575135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39560</th>\n",
       "      <td>imapotato_ouo</td>\n",
       "      <td>-2.472750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18889</th>\n",
       "      <td>brianmolko_off</td>\n",
       "      <td>-2.465924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182492</th>\n",
       "      <td>пичалька</td>\n",
       "      <td>-2.462784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69160</th>\n",
       "      <td>plo_otnik</td>\n",
       "      <td>-2.422393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122076</th>\n",
       "      <td>грустный</td>\n",
       "      <td>-2.415715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51154</th>\n",
       "      <td>leto_on_mars</td>\n",
       "      <td>-2.401907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171825</th>\n",
       "      <td>огорчает</td>\n",
       "      <td>-2.392797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122064</th>\n",
       "      <td>грустно</td>\n",
       "      <td>-2.386727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12738</th>\n",
       "      <td>angelino_ochka</td>\n",
       "      <td>-2.368943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142430</th>\n",
       "      <td>кайфую</td>\n",
       "      <td>1.822264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126524</th>\n",
       "      <td>довольная</td>\n",
       "      <td>1.823796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225502</th>\n",
       "      <td>угадал</td>\n",
       "      <td>1.834453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190098</th>\n",
       "      <td>постепенно</td>\n",
       "      <td>1.840036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140553</th>\n",
       "      <td>интересная</td>\n",
       "      <td>1.865426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91162</th>\n",
       "      <td>weasly_fred</td>\n",
       "      <td>1.877303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218337</th>\n",
       "      <td>счастлива</td>\n",
       "      <td>1.886828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238584</th>\n",
       "      <td>шикарное</td>\n",
       "      <td>1.896283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234118</th>\n",
       "      <td>хочупровестиновогоднююночьвместес</td>\n",
       "      <td>1.906590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191002</th>\n",
       "      <td>похудей</td>\n",
       "      <td>1.916453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225884</th>\n",
       "      <td>удачного</td>\n",
       "      <td>1.933872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206884</th>\n",
       "      <td>свершилось</td>\n",
       "      <td>1.941334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192456</th>\n",
       "      <td>прекрасного</td>\n",
       "      <td>1.943709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175508</th>\n",
       "      <td>отдыхаем</td>\n",
       "      <td>1.948733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194992</th>\n",
       "      <td>пробовать</td>\n",
       "      <td>1.975722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43333</th>\n",
       "      <td>juljulianapai</td>\n",
       "      <td>1.979446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203503</th>\n",
       "      <td>ржу</td>\n",
       "      <td>1.986723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184991</th>\n",
       "      <td>поднимешься</td>\n",
       "      <td>2.009854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35190</th>\n",
       "      <td>gregwest_</td>\n",
       "      <td>2.126458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194803</th>\n",
       "      <td>приятная</td>\n",
       "      <td>2.128157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226929</th>\n",
       "      <td>улыбнуло</td>\n",
       "      <td>2.158935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187147</th>\n",
       "      <td>полезно</td>\n",
       "      <td>2.222933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211451</th>\n",
       "      <td>смеха</td>\n",
       "      <td>2.235688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132281</th>\n",
       "      <td>забавно</td>\n",
       "      <td>2.276834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175964</th>\n",
       "      <td>отличного</td>\n",
       "      <td>2.542094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24245</th>\n",
       "      <td>ddddd</td>\n",
       "      <td>2.697284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24243</th>\n",
       "      <td>dddd</td>\n",
       "      <td>3.421041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92631</th>\n",
       "      <td>xd</td>\n",
       "      <td>3.447770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24242</th>\n",
       "      <td>ddd</td>\n",
       "      <td>3.884956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24231</th>\n",
       "      <td>dd</td>\n",
       "      <td>4.478038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243421 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   fnames         w\n",
       "64845                                 o_o -5.485647\n",
       "21179                         cio_optimal -4.407953\n",
       "84049                        to_over_kill -4.373000\n",
       "26275                       do_or_die_xxx -4.166842\n",
       "70243                         prisonero_o -4.156347\n",
       "37639                          horanso_on -3.684225\n",
       "72988                           reno_oppa -3.670594\n",
       "7130                         _do_or_die__ -3.553882\n",
       "7833                           _ooo_ooo__ -3.451456\n",
       "64850                          o_obnulyay -3.403966\n",
       "18514                            boo_ohoo -3.403440\n",
       "169691                                о_о -3.372854\n",
       "47678                          kota_oo_oo -3.285324\n",
       "72178                       radio_of_moon -3.257457\n",
       "53242                      lponomarenko_o -2.982029\n",
       "170084                             обидно -2.814166\n",
       "181498                           печально -2.734797\n",
       "43355                     july_to_october -2.709523\n",
       "6125                                   99 -2.640723\n",
       "131530                          жизньболь -2.598712\n",
       "64860                            o_olshik -2.575135\n",
       "39560                       imapotato_ouo -2.472750\n",
       "18889                      brianmolko_off -2.465924\n",
       "182492                           пичалька -2.462784\n",
       "69160                           plo_otnik -2.422393\n",
       "122076                           грустный -2.415715\n",
       "51154                        leto_on_mars -2.401907\n",
       "171825                           огорчает -2.392797\n",
       "122064                            грустно -2.386727\n",
       "12738                      angelino_ochka -2.368943\n",
       "...                                   ...       ...\n",
       "142430                             кайфую  1.822264\n",
       "126524                          довольная  1.823796\n",
       "225502                             угадал  1.834453\n",
       "190098                         постепенно  1.840036\n",
       "140553                         интересная  1.865426\n",
       "91162                         weasly_fred  1.877303\n",
       "218337                          счастлива  1.886828\n",
       "238584                           шикарное  1.896283\n",
       "234118  хочупровестиновогоднююночьвместес  1.906590\n",
       "191002                            похудей  1.916453\n",
       "225884                           удачного  1.933872\n",
       "206884                         свершилось  1.941334\n",
       "192456                        прекрасного  1.943709\n",
       "175508                           отдыхаем  1.948733\n",
       "194992                          пробовать  1.975722\n",
       "43333                       juljulianapai  1.979446\n",
       "203503                                ржу  1.986723\n",
       "184991                        поднимешься  2.009854\n",
       "35190                           gregwest_  2.126458\n",
       "194803                           приятная  2.128157\n",
       "226929                           улыбнуло  2.158935\n",
       "187147                            полезно  2.222933\n",
       "211451                              смеха  2.235688\n",
       "132281                            забавно  2.276834\n",
       "175964                          отличного  2.542094\n",
       "24245                               ddddd  2.697284\n",
       "24243                                dddd  3.421041\n",
       "92631                                  xd  3.447770\n",
       "24242                                 ddd  3.884956\n",
       "24231                                  dd  4.478038\n",
       "\n",
       "[243421 rows x 2 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames = vec.get_feature_names()\n",
    "weights = clf.coef_[0]\n",
    "pd.DataFrame({\"w\":weights, \"fnames\":fnames}).sort_values(\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуйте использовать следующие модификации модели:\n",
    "* Используйте Tf-Idf\n",
    "* Используйте токенизатор и удаление стоп-слов (векторизатор уже создан ниже) \n",
    "* Обучите модель на биграммах и триграммах\n",
    "\n",
    "(рекомендуется написать функцию, которая берет на вход токенизатор и возвращает модель)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_everything(vec):\n",
    "    x_train_bow = vec.fit_transform(x_train) # bow -- bag of words (мешок слов)\n",
    "    x_test_bow = vec.transform(x_test)\n",
    "    print(\"Число признаков:\", x_train_bow.shape[1])\n",
    "    print(\"Пример слов:\", vec.get_feature_names()[579:589])\n",
    "    clf = LogisticRegression(max_iter=300, random_state=42)\n",
    "    clf.fit(x_train_bow, y_train)\n",
    "    pred = clf.predict(vec.transform(x_test))\n",
    "    print('ACC: %.3f' % \\\n",
    "          accuracy_score(y_test, pred))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число признаков: 243421\n",
      "Пример слов: ['100р', '100респектов', '100тыс', '100хан', '101', '10110000', '102', '1020', '1023зс', '103']\n",
      "ACC: 0.758\n"
     ]
    }
   ],
   "source": [
    "# Tf-Idf\n",
    "vec = TfidfVectorizer(ngram_range=(1, 1))\n",
    "clf = do_everything(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число признаков: 82090\n",
      "Пример слов: ['3oae2fgxba', '3qruyahwqz', '3slona', '3tns5amgog', '3v8suc04u7', '3wazrgzycn', '3xl', '3xqdra27md', '3z54s8absp', '3zc7exqlcw']\n",
      "ACC: 0.760\n"
     ]
    }
   ],
   "source": [
    "# Count + min_df\n",
    "vec = CountVectorizer(ngram_range=(1, 1), min_df=2)\n",
    "clf = do_everything(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число признаков: 82090\n",
      "Пример слов: ['3oae2fgxba', '3qruyahwqz', '3slona', '3tns5amgog', '3v8suc04u7', '3wazrgzycn', '3xl', '3xqdra27md', '3z54s8absp', '3zc7exqlcw']\n",
      "ACC: 0.758\n"
     ]
    }
   ],
   "source": [
    "# Tfidf + min_df\n",
    "vec = TfidfVectorizer(ngram_range=(1, 1), min_df=2)\n",
    "clf = do_everything(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число признаков: 266053\n",
      "Пример слов: ['-13ºс', '-14', '-15', '-15-16', '-15.', '-15..', '-1500', '-15=0', '-16', '-17']\n",
      "ACC: 0.780\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1), \\\n",
    "                      tokenizer=word_tokenize, \\\n",
    "                      stop_words=noise)\n",
    "clf = do_everything(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число признаков: 29448\n",
      "Пример слов: ['464raza', '47', '48', '49', '4:0.', '4:30', '4attyakatilla', '4g', '4post', '4s']\n",
      "ACC: 0.767\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(ngram_range=(1, 1), \\\n",
    "                      tokenizer=word_tokenize, \\\n",
    "                      stop_words=noise,\\\n",
    "                      min_df=5)\n",
    "clf = do_everything(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число признаков: 1003360\n",
      "Пример слов: ['09 2013', '09 26', '09 30', '09 69', '09 http', '09 вас', '09 дек', '09 когда', '09 мне', '09 не']\n",
      "ACC: 0.707\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(2, 2))\n",
    "clf = do_everything(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Число признаков: 1328679\n",
      "Пример слов: ['04 на 10', '04 на всякий', '04 нельзя обновить', '04 ни gnome', '04 орлец не', '04 рисую совсем', '04 россия вам', '04 сразу апгрейдиться', '04 такое месево', '04032379 0060 спасибо']\n",
      "ACC: 0.648\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(ngram_range=(3, 3))\n",
    "clf = do_everything(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тем, кто все сделал, рекомендуется посмотреть [соревнование майнора](https://www.kaggle.com/t/9f493c11e0b24931996e5a8aec870a49) про классификацию объявлений на категории. На странице соревнования выложено [базовое решение](https://colab.research.google.com/drive/14PeTbhiOinYy_7aonQB-8bCAZqOIC-VD#scrollTo=HXWGHGdhzTT7), можно его скачать и выполнить простейшие модификации, например те же, что в этом задании."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
